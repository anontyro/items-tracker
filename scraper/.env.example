# Scraper service example environment
# Copy to .env and adjust values for local development

SCRAPE_SCHEDULE=0 0 * * *
RATE_LIMIT_DELAY=2000
MAX_RETRIES=2
RETRY_DELAY=5000
BACKEND_API_URL=http://localhost:3005
# Must match SCRAPER_API_KEY in backend/.env
API_KEY=change-me
LOG_LEVEL=info

# Optional: Playwright browser settings
# Set to false or 0 to run in headed mode for debugging
PLAYWRIGHT_HEADLESS=true
# Optional: override the default user agent used by Playwright
# PLAYWRIGHT_USER_AGENT=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36

# Optional: limit number of pages per site scrape (0 or unset = all pages)
SCRAPER_MAX_PAGES=0

# Optional: max snapshots per batch when sending to backend API
# SCRAPER_BACKEND_BATCH_SIZE=50

# Optional: override path to local SQLite database file for raw scrape storage
# Default is ./scraper-data.sqlite in the scraper working directory
SCRAPER_SQLITE_PATH=./scraper-data.sqlite

# Optional: disable local SQLite persistence (useful if better-sqlite3 cannot build)
# SCRAPER_DISABLE_SQLITE=true

# Optional: sync worker and manual sync configuration
# If unset, the sync worker and CLI default to BACKEND_API_URL and API_KEY above.
# Use these to point at a different environment (e.g. production) when draining the queue.
# SYNC_API_URL=https://prod-api.example.com
# SYNC_API_KEY=change-me-prod

# Optional: background worker tuning
# Interval between polling loops in milliseconds (default: 60000)
# SYNC_WORKER_INTERVAL_MS=60000
# Maximum queue rows to process per loop (default: 50)
# SYNC_WORKER_BATCH_LIMIT=50

# Optional: run scraper in long-lived service mode (default: false)
# When enabled, the process stays alive after the initial run to allow future
# job scheduling / workers to be attached.
# SCRAPER_SERVICE_MODE=true

# Optional: enable detail images (default: false)
SCRAPER_ENABLE_DETAIL_IMAGES=false